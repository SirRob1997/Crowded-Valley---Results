{
    "testproblem": "mnist_vae",
    "batch_size": 64,
    "num_epochs": 50,
    "random_seed": 42,
    "l2_reg": null,
    "optimizer_name": "AdaBoundOptimizer",
    "optimizer_hyperparams": {
        "learning_rate": 0.13826232179369854,
        "final_lr": 0.003458752914024413,
        "beta1": 0.713778471879828,
        "beta2": 0.9125364766720501,
        "gamma": 0.00013783237455007176,
        "epsilon": 1e-08,
        "amsbound": true,
        "use_locking": false
    },
    "training_params": {
        "lr_sched_epochs": [
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31,
            32,
            33,
            34,
            35,
            36,
            37,
            38,
            39,
            40,
            41,
            42,
            43,
            44,
            45,
            46,
            47,
            48,
            49,
            50
        ],
        "lr_sched_factors": [
            0.9990133642141358,
            0.996057350657239,
            0.9911436253643444,
            0.9842915805643155,
            0.9755282581475768,
            0.9648882429441257,
            0.9524135262330098,
            0.9381533400219317,
            0.9221639627510075,
            0.9045084971874737,
            0.8852566213878946,
            0.8644843137107058,
            0.8422735529643444,
            0.8187119948743449,
            0.7938926261462366,
            0.7679133974894983,
            0.7408768370508576,
            0.7128896457825363,
            0.6840622763423391,
            0.6545084971874737,
            0.6243449435824275,
            0.5936906572928624,
            0.5626666167821521,
            0.5313952597646567,
            0.5,
            0.4686047402353433,
            0.4373333832178478,
            0.4063093427071376,
            0.3756550564175727,
            0.34549150281252633,
            0.31593772365766104,
            0.28711035421746367,
            0.2591231629491423,
            0.23208660251050156,
            0.2061073738537635,
            0.18128800512565513,
            0.15772644703565564,
            0.13551568628929433,
            0.11474337861210543,
            0.09549150281252633,
            0.07783603724899257,
            0.06184665997806832,
            0.04758647376699032,
            0.035111757055874326,
            0.024471741852423234,
            0.015708419435684462,
            0.008856374635655695,
            0.0039426493427611176,
            0.0009866357858642205,
            0.0
        ]
    },
    "train_losses": [
        181.38856428097455,
        181.38856428097455,
        181.38856428097455,
        181.38856428097455,
        181.38856428097455,
        181.38856428097455,
        181.38856428097455,
        181.38856428097455,
        181.38856428097455,
        181.38856428097455,
        181.38856428097455,
        181.38856428097455,
        181.38856428097455,
        181.38856428097455,
        181.38856428097455,
        181.38856428097455,
        181.38856428097455,
        181.38856428097455,
        181.38856428097455,
        181.38856428097455,
        181.38856428097455,
        181.38856428097455,
        181.38856428097455,
        181.38856428097455,
        181.38856428097455,
        181.38856428097455,
        181.38856428097455,
        181.38856428097455,
        181.38856428097455,
        181.38856428097455,
        181.38856428097455,
        181.38856428097455,
        181.38856428097455,
        181.38856428097455,
        181.38856428097455,
        181.38856428097455,
        181.38856428097455,
        181.38856428097455,
        181.38856428097455,
        181.38856428097455,
        181.38856428097455,
        181.38856428097455,
        181.38856428097455,
        181.38856428097455,
        181.38856428097455,
        181.38856428097455,
        181.38856428097455,
        181.38856428097455,
        181.38856428097455,
        181.38856428097455,
        181.38856428097455
    ],
    "valid_losses": [
        181.33862500313,
        181.33862500313,
        181.33862500313,
        181.33862500313,
        181.33862500313,
        181.33862500313,
        181.33862500313,
        181.33862500313,
        181.33862500313,
        181.33862500313,
        181.33862500313,
        181.33862500313,
        181.33862500313,
        181.33862500313,
        181.33862500313,
        181.33862500313,
        181.33862500313,
        181.33862500313,
        181.33862500313,
        181.33862500313,
        181.33862500313,
        181.33862500313,
        181.33862500313,
        181.33862500313,
        181.33862500313,
        181.33862500313,
        181.33862500313,
        181.33862500313,
        181.33862500313,
        181.33862500313,
        181.33862500313,
        181.33862500313,
        181.33862500313,
        181.33862500313,
        181.33862500313,
        181.33862500313,
        181.33862500313,
        181.33862500313,
        181.33862500313,
        181.33862500313,
        181.33862500313,
        181.33862500313,
        181.33862500313,
        181.33862500313,
        181.33862500313,
        181.33862500313,
        181.33862500313,
        181.33862500313,
        181.33862500313,
        181.33862500313,
        181.33862500313
    ],
    "test_losses": [
        181.4022609026004,
        181.4022609026004,
        181.4022609026004,
        181.4022609026004,
        181.4022609026004,
        181.4022609026004,
        181.4022609026004,
        181.4022609026004,
        181.4022609026004,
        181.4022609026004,
        181.4022609026004,
        181.4022609026004,
        181.4022609026004,
        181.4022609026004,
        181.4022609026004,
        181.4022609026004,
        181.4022609026004,
        181.4022609026004,
        181.4022609026004,
        181.4022609026004,
        181.4022609026004,
        181.4022609026004,
        181.4022609026004,
        181.4022609026004,
        181.4022609026004,
        181.4022609026004,
        181.4022609026004,
        181.4022609026004,
        181.4022609026004,
        181.4022609026004,
        181.4022609026004,
        181.4022609026004,
        181.4022609026004,
        181.4022609026004,
        181.4022609026004,
        181.4022609026004,
        181.4022609026004,
        181.4022609026004,
        181.4022609026004,
        181.4022609026004,
        181.4022609026004,
        181.4022609026004,
        181.4022609026004,
        181.4022609026004,
        181.4022609026004,
        181.4022609026004,
        181.4022609026004,
        181.4022609026004,
        181.4022609026004,
        181.4022609026004,
        181.4022609026004
    ],
    "minibatch_train_losses": [
        181.1411590576172,
        181.1411590576172,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        NaN,
        181.1411590576172,
        181.1411590576172,
        181.1411590576172,
        181.1411590576172,
        181.1411590576172,
        181.1411590576172,
        181.1411590576172,
        181.1411590576172,
        181.1411590576172,
        181.1411590576172,
        181.1411590576172,
        181.1411590576172,
        181.1411590576172,
        181.1411590576172,
        181.1411590576172,
        181.1411590576172,
        181.1411590576172,
        181.1411590576172,
        181.1411590576172,
        181.1411590576172,
        181.1411590576172,
        181.1411590576172,
        181.1411590576172,
        181.1411590576172,
        181.1411590576172,
        181.1411590576172,
        181.1411590576172,
        181.1411590576172,
        181.1411590576172,
        181.1411590576172,
        181.1411590576172,
        181.1411590576172,
        181.1411590576172,
        181.1411590576172,
        181.1411590576172,
        181.1411590576172,
        181.1411590576172,
        181.1411590576172,
        181.1411590576172,
        181.1411590576172,
        181.1411590576172,
        181.1411590576172,
        181.1411590576172,
        181.1411590576172,
        181.1411590576172,
        181.1411590576172,
        181.1411590576172,
        181.1411590576172,
        181.1411590576172,
        181.1411590576172
    ]
}